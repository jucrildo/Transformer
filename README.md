# Transformer

## Decoder-only transformer that generates text

Implementation of a decoder-only transformer trained on tinyshakespeare dataset that was built following Andrej Karpathy youtube series. This transformer follows the architecture of a gpt-2 model, without fine-tuning. The project was mainly based on Karpathy code, but is still under development, so eventually will have new features and other transformer techiniques that are not on the tutorials.

The main objective of this project is purely educational, so it's very basic and simple. 

## TODO
- [x] MultiHead attention in parallel
- [ ] Implement distributed programming operations
- [ ] Data parallelism
- [ ] Speed-up training
